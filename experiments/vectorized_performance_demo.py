"""
Comprehensive performance demonstration script for vectorized Layer 2 improvements.

This script validates and showcases the performance benefits of the vectorized
cultural transmission implementation compared to the sequential approach.
Measures execution time, validates mathematical equivalence, and generates
performance visualizations.
"""

from __future__ import annotations

import argparse
import json
import logging
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import matplotlib.pyplot as plt
import numpy as np
from beartype import beartype

from lovebug.lande_kirkpatrick import LandeKirkpatrickParams
from lovebug.layer2.config import Layer2Config
from lovebug.layer_activation import LayerActivationConfig
from lovebug.unified_mesa_model import UnifiedLoveModel

__all__ = ["PerformanceBenchmark", "run_performance_demo"]

logger = logging.getLogger(__name__)


@dataclass(slots=True, frozen=False)
class BenchmarkResult:
    """Results from a single benchmark run."""

    population_size: int
    implementation: str  # "vectorized" or "sequential"
    generations: int
    total_time: float
    time_per_generation: float
    time_per_cultural_step: float
    cultural_learning_events: int
    cultural_innovation_events: int
    final_cultural_diversity: float
    final_cultural_preference_mean: float
    final_cultural_preference_var: float
    memory_usage_mb: float = 0.0


@dataclass(slots=True, frozen=False)
class PerformanceComparison:
    """Comparison between vectorized and sequential implementations."""

    population_size: int
    vectorized_result: BenchmarkResult
    sequential_result: BenchmarkResult
    speedup_ratio: float = field(init=False)
    time_improvement: float = field(init=False)
    mathematical_equivalence: bool = field(init=False)

    def __post_init__(self) -> None:
        """Calculate derived metrics."""
        self.speedup_ratio = (
            self.sequential_result.time_per_cultural_step / self.vectorized_result.time_per_cultural_step
        )
        self.time_improvement = (
            (self.sequential_result.total_time - self.vectorized_result.total_time)
            / self.sequential_result.total_time
            * 100
        )

        # Check mathematical equivalence (within tolerance)
        pref_diff = abs(
            self.vectorized_result.final_cultural_preference_mean
            - self.sequential_result.final_cultural_preference_mean
        )
        var_diff = abs(
            self.vectorized_result.final_cultural_preference_var - self.sequential_result.final_cultural_preference_var
        )

        # Allow 5% tolerance for stochastic differences
        self.mathematical_equivalence = pref_diff < 0.05 * max(
            self.vectorized_result.final_cultural_preference_mean, self.sequential_result.final_cultural_preference_mean
        ) and var_diff < 0.05 * max(
            self.vectorized_result.final_cultural_preference_var, self.sequential_result.final_cultural_preference_var
        )


class PerformanceBenchmark:
    """
    Comprehensive performance benchmark for vectorized Layer 2 implementation.

    Compares vectorized vs sequential cultural transmission performance across
    multiple population sizes with statistical validation.
    """

    def __init__(
        self,
        population_sizes: list[int] | None = None,
        generations: int = 50,
        runs_per_config: int = 3,
        output_dir: str | Path = "experiments/performance_results",
    ) -> None:
        """
        Initialize performance benchmark.

        Parameters
        ----------
        population_sizes : list[int] | None
            Population sizes to test (default: [100, 500, 1000, 2000, 5000])
        generations : int
            Number of generations per run
        runs_per_config : int
            Number of runs per configuration for statistical validity
        output_dir : str | Path
            Directory to save results and plots
        """
        self.population_sizes = population_sizes or [100, 500, 1000, 2000, 5000]
        self.generations = generations
        self.runs_per_config = runs_per_config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Results storage
        self.benchmark_results: list[BenchmarkResult] = []
        self.performance_comparisons: list[PerformanceComparison] = []

        logger.info(
            f"Initialized benchmark: populations={self.population_sizes}, "
            f"generations={self.generations}, runs={self.runs_per_config}"
        )

    @beartype
    def _create_test_configuration(self) -> tuple[LayerActivationConfig, LandeKirkpatrickParams, Layer2Config]:
        """Create standardized test configuration for benchmarking."""
        # Cultural-only configuration for focused Layer 2 testing
        layer_config = LayerActivationConfig.cultural_only()

        # Basic genetic parameters (not used in cultural-only mode)
        genetic_params = LandeKirkpatrickParams(mutation_variance=1e-4, selection_strength=0.1, genetic_correlation=0.1)

        # Realistic cultural parameters for performance testing
        cultural_params = Layer2Config(
            oblique_transmission_rate=0.3,
            horizontal_transmission_rate=0.2,
            innovation_rate=0.05,
            network_type="small_world",
            network_connectivity=0.1,
            local_learning_radius=5,
            cultural_memory_size=5,
            memory_decay_rate=0.05,
            memory_update_strength=1.0,
            log_cultural_events=False,  # Disable for performance
            save_detailed_data=False,
        )

        return layer_config, genetic_params, cultural_params

    @beartype
    def _run_single_benchmark(self, population_size: int, use_vectorized: bool, run_id: int) -> BenchmarkResult:
        """
        Run a single benchmark with specified configuration.

        Parameters
        ----------
        population_size : int
            Number of agents in the population
        use_vectorized : bool
            Whether to use vectorized implementation
        run_id : int
            Run identifier for logging

        Returns
        -------
        BenchmarkResult
            Benchmark results for this run
        """
        implementation = "vectorized" if use_vectorized else "sequential"
        logger.info(f"Running {implementation} benchmark: N={population_size}, run {run_id + 1}/{self.runs_per_config}")

        # Create configuration
        layer_config, genetic_params, cultural_params = self._create_test_configuration()

        # Create model with specified vectorization setting
        model = UnifiedLoveModel(
            layer_config=layer_config,
            genetic_params=genetic_params,
            cultural_params=cultural_params,
            n_agents=population_size,
            use_vectorized_cultural_layer=use_vectorized,
        )

        # Measure execution time
        start_time = time.perf_counter()
        results = model.run(n_steps=self.generations)
        end_time = time.perf_counter()

        total_time = end_time - start_time
        time_per_generation = total_time / self.generations

        # Extract cultural learning statistics
        cultural_events = sum(h.get("cultural_learning_events", 0) for h in model.history)
        innovation_events = sum(h.get("cultural_innovation_events", 0) for h in model.history)

        # Calculate time per cultural step (approximate)
        total_cultural_steps = cultural_events + innovation_events
        time_per_cultural_step = total_time / max(1, total_cultural_steps)

        # Final cultural state
        final_metrics = results.get("final_metrics", {})
        cultural_diversity = self._calculate_cultural_diversity(model)

        return BenchmarkResult(
            population_size=population_size,
            implementation=implementation,
            generations=self.generations,
            total_time=total_time,
            time_per_generation=time_per_generation,
            time_per_cultural_step=time_per_cultural_step,
            cultural_learning_events=cultural_events,
            cultural_innovation_events=innovation_events,
            final_cultural_diversity=cultural_diversity,
            final_cultural_preference_mean=final_metrics.get("mean_cultural_preference", 0.0),
            final_cultural_preference_var=final_metrics.get("var_cultural_preference", 0.0),
            memory_usage_mb=0.0,  # Could add memory profiling if needed
        )

    def _calculate_cultural_diversity(self, model: UnifiedLoveModel) -> float:
        """Calculate Shannon diversity of cultural preferences."""
        if len(model.agents) == 0:
            return 0.0

        try:
            # Get cultural preferences from the vectorized layer if available
            agent_set = model.agents._agentsets[0]
            if hasattr(agent_set, "vectorized_cultural_layer") and agent_set.vectorized_cultural_layer:
                return agent_set.vectorized_cultural_layer.compute_cultural_diversity()

            # Fallback to DataFrame calculation
            df = agent_set.agents
            if "pref_culture" not in df.columns:
                return 0.0

            pref_counts = df.group_by("pref_culture").len()
            if len(pref_counts) <= 1:
                return 0.0

            counts = pref_counts.get_column("len").to_numpy()
            total = np.sum(counts)
            proportions = counts / total

            # Shannon entropy
            diversity = -np.sum(proportions * np.log(proportions + 1e-10))
            return float(diversity)

        except Exception as e:
            logger.warning(f"Could not calculate cultural diversity: {e}")
            return 0.0

    @beartype
    def run_full_benchmark(self) -> None:
        """
        Run the complete performance benchmark across all configurations.

        This method executes the benchmark for all population sizes and both
        implementations, collecting comprehensive performance data.
        """
        logger.info("Starting comprehensive performance benchmark")

        for pop_size in self.population_sizes:
            logger.info(f"Benchmarking population size: {pop_size}")

            # Run vectorized benchmarks
            vectorized_results = []
            for run_id in range(self.runs_per_config):
                try:
                    result = self._run_single_benchmark(pop_size, use_vectorized=True, run_id=run_id)
                    vectorized_results.append(result)
                    self.benchmark_results.append(result)
                except Exception as e:
                    logger.error(f"Vectorized benchmark failed for N={pop_size}, run {run_id}: {e}")
                    continue

            # Run sequential benchmarks
            sequential_results = []
            for run_id in range(self.runs_per_config):
                try:
                    result = self._run_single_benchmark(pop_size, use_vectorized=False, run_id=run_id)
                    sequential_results.append(result)
                    self.benchmark_results.append(result)
                except Exception as e:
                    logger.error(f"Sequential benchmark failed for N={pop_size}, run {run_id}: {e}")
                    continue

            # Create performance comparison (using best runs)
            if vectorized_results and sequential_results:
                # Use the fastest run for each implementation
                best_vectorized = min(vectorized_results, key=lambda r: r.total_time)
                best_sequential = min(sequential_results, key=lambda r: r.total_time)

                comparison = PerformanceComparison(
                    population_size=pop_size, vectorized_result=best_vectorized, sequential_result=best_sequential
                )
                self.performance_comparisons.append(comparison)

                logger.info(
                    f"N={pop_size}: Speedup={comparison.speedup_ratio:.2f}x, "
                    f"Time improvement={comparison.time_improvement:.1f}%, "
                    f"Math equivalent={comparison.mathematical_equivalence}"
                )

        logger.info("Performance benchmark completed")

    def _create_diagnostic_report(self) -> None:
        """Create a diagnostic report when vectorized implementation fails."""
        logger.info("Creating diagnostic report for vectorized implementation issues")

        # Analyze sequential results
        sequential_results = [r for r in self.benchmark_results if r.implementation == "sequential"]
        if not sequential_results:
            logger.warning("No successful runs to analyze")
            return

        print("\n" + "=" * 80)
        print("DIAGNOSTIC REPORT: VECTORIZED IMPLEMENTATION ISSUES")
        print("=" * 80)
        print("\nISSUE SUMMARY:")
        print("- Vectorized Layer 2 implementation is encountering type compatibility errors")
        print("- Error: 'type Int64 is incompatible with expected type UInt32'")
        print("- Sequential implementation is working correctly")
        print("- This indicates a data type casting issue in the vectorized layer")

        print("\nSEQUENTIAL PERFORMANCE ANALYSIS:")
        print(f"{'Size':<8} {'Time(s)':<12} {'Events':<10} {'Time/Event':<12}")
        print("-" * 50)

        for result in sequential_results:
            time_per_event = result.time_per_cultural_step
            print(
                f"{result.population_size:<8} "
                f"{result.total_time:<12.3f} "
                f"{result.cultural_learning_events:<10} "
                f"{time_per_event:<12.6f}"
            )

        print("\nRECOMMENDATIONS:")
        print("1. Fix type casting issues in VectorizedCulturalLayer")
        print("2. Ensure agent_id columns use consistent UInt32 types")
        print("3. Review polars DataFrame schema alignment")
        print("4. Run tests: uv run pytest tests/test_vectorized_layer2.py -v")
        print("5. Re-run benchmark after fixes")
        print("=" * 80)

    @beartype
    def generate_performance_report(self) -> dict[str, Any]:
        """
        Generate comprehensive performance report.

        Returns
        -------
        dict[str, Any]
            Complete performance analysis report
        """
        if not self.performance_comparisons:
            # Check if we have any results at all
            if not self.benchmark_results:
                raise ValueError("No benchmark results available. Run benchmark first.")

            # We have results but no comparisons - create a diagnostic report
            sequential_results = [r for r in self.benchmark_results if r.implementation == "sequential"]
            vectorized_results = [r for r in self.benchmark_results if r.implementation == "vectorized"]

            return {
                "benchmark_summary": {
                    "population_sizes": self.population_sizes,
                    "generations_per_run": self.generations,
                    "runs_per_configuration": self.runs_per_config,
                    "total_benchmark_runs": len(self.benchmark_results),
                    "successful_comparisons": 0,
                    "vectorized_runs_successful": len(vectorized_results),
                    "sequential_runs_successful": len(sequential_results),
                },
                "performance_results": {
                    "mean_speedup": 0.0,
                    "max_speedup": 0.0,
                    "min_speedup": 0.0,
                    "mean_time_improvement_percent": 0.0,
                    "mathematical_equivalence_rate": 0.0,
                },
                "detailed_comparisons": [],
                "validation_results": {
                    "all_runs_mathematically_equivalent": False,
                    "performance_improvements_consistent": False,
                    "expected_performance_gains_achieved": False,
                    "vectorized_implementation_status": "FAILED"
                    if len(vectorized_results) == 0
                    else "PARTIALLY_FAILED",
                },
                "diagnostic_info": {
                    "issue": "Vectorized implementation failing with type compatibility errors",
                    "error_pattern": "type Int64 is incompatible with expected type UInt32",
                    "sequential_working": len(sequential_results) > 0,
                    "recommendation": "Fix type casting in VectorizedCulturalLayer",
                },
            }

        # Calculate summary statistics
        speedups = [comp.speedup_ratio for comp in self.performance_comparisons]
        improvements = [comp.time_improvement for comp in self.performance_comparisons]
        equivalence_rate = sum(comp.mathematical_equivalence for comp in self.performance_comparisons) / len(
            self.performance_comparisons
        )

        report = {
            "benchmark_summary": {
                "population_sizes": self.population_sizes,
                "generations_per_run": self.generations,
                "runs_per_configuration": self.runs_per_config,
                "total_benchmark_runs": len(self.benchmark_results),
                "successful_comparisons": len(self.performance_comparisons),
            },
            "performance_results": {
                "mean_speedup": float(np.mean(speedups)),
                "max_speedup": float(np.max(speedups)),
                "min_speedup": float(np.min(speedups)),
                "mean_time_improvement_percent": float(np.mean(improvements)),
                "mathematical_equivalence_rate": float(equivalence_rate),
            },
            "detailed_comparisons": [
                {
                    "population_size": comp.population_size,
                    "speedup_ratio": comp.speedup_ratio,
                    "time_improvement_percent": comp.time_improvement,
                    "vectorized_time": comp.vectorized_result.total_time,
                    "sequential_time": comp.sequential_result.total_time,
                    "mathematical_equivalence": comp.mathematical_equivalence,
                    "vectorized_cultural_events": comp.vectorized_result.cultural_learning_events,
                    "sequential_cultural_events": comp.sequential_result.cultural_learning_events,
                }
                for comp in self.performance_comparisons
            ],
            "validation_results": {
                "all_runs_mathematically_equivalent": equivalence_rate == 1.0,
                "performance_improvements_consistent": all(s > 1.0 for s in speedups),
                "expected_performance_gains_achieved": np.mean(speedups) >= 2.0,  # Expect at least 2x
            },
        }

        return report

    @beartype
    def save_results(self, filename: str = "performance_benchmark_results.json") -> None:
        """Save benchmark results to JSON file."""
        results_file = self.output_dir / filename

        # Convert results to serializable format
        results_data = {
            "benchmark_results": [
                {
                    "population_size": r.population_size,
                    "implementation": r.implementation,
                    "generations": r.generations,
                    "total_time": r.total_time,
                    "time_per_generation": r.time_per_generation,
                    "time_per_cultural_step": r.time_per_cultural_step,
                    "cultural_learning_events": r.cultural_learning_events,
                    "cultural_innovation_events": r.cultural_innovation_events,
                    "final_cultural_diversity": r.final_cultural_diversity,
                    "final_cultural_preference_mean": r.final_cultural_preference_mean,
                    "final_cultural_preference_var": r.final_cultural_preference_var,
                    "memory_usage_mb": r.memory_usage_mb,
                }
                for r in self.benchmark_results
            ],
            "performance_report": self.generate_performance_report(),
        }

        with results_file.open("w") as f:
            json.dump(results_data, f, indent=2)

        logger.info(f"Results saved to {results_file}")

    @beartype
    def plot_performance_comparison(self) -> None:
        """Generate performance comparison visualizations."""
        if not self.performance_comparisons:
            raise ValueError("No benchmark results available. Run benchmark first.")

        # Extract data for plotting
        population_sizes = [comp.population_size for comp in self.performance_comparisons]
        speedup_ratios = [comp.speedup_ratio for comp in self.performance_comparisons]
        vectorized_times = [comp.vectorized_result.total_time for comp in self.performance_comparisons]
        sequential_times = [comp.sequential_result.total_time for comp in self.performance_comparisons]

        # Create comprehensive performance plots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("Vectorized Layer 2 Performance Analysis", fontsize=16, fontweight="bold")

        # Plot 1: Speedup ratios
        ax1.plot(population_sizes, speedup_ratios, "bo-", linewidth=2, markersize=8)
        ax1.axhline(y=1.0, color="r", linestyle="--", alpha=0.7, label="No improvement")
        ax1.set_xlabel("Population Size")
        ax1.set_ylabel("Speedup Ratio (Sequential/Vectorized)")
        ax1.set_title("Performance Speedup by Population Size")
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        ax1.set_xscale("log")

        # Add speedup annotations
        for _i, (pop, speedup) in enumerate(zip(population_sizes, speedup_ratios)):
            ax1.annotate(f"{speedup:.1f}x", (pop, speedup), textcoords="offset points", xytext=(0, 10), ha="center")

        # Plot 2: Execution time comparison
        width = 0.35
        x_pos = np.arange(len(population_sizes))

        ax2.bar(x_pos - width / 2, vectorized_times, width, label="Vectorized", alpha=0.8, color="green")
        ax2.bar(x_pos + width / 2, sequential_times, width, label="Sequential", alpha=0.8, color="red")
        ax2.set_xlabel("Population Size")
        ax2.set_ylabel("Execution Time (seconds)")
        ax2.set_title("Execution Time Comparison")
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels([str(p) for p in population_sizes])
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Plot 3: Scaling behavior
        ax3.loglog(population_sizes, vectorized_times, "go-", label="Vectorized", linewidth=2, markersize=8)
        ax3.loglog(population_sizes, sequential_times, "ro-", label="Sequential", linewidth=2, markersize=8)

        # Add theoretical scaling lines
        if len(population_sizes) >= 2:
            # O(n) scaling reference
            n_min = min(population_sizes)
            t_min = min(min(vectorized_times), min(sequential_times))
            linear_ref = [t_min * (n / n_min) for n in population_sizes]
            ax3.loglog(population_sizes, linear_ref, "k--", alpha=0.5, label="O(n) reference")

            # O(n¬≤) scaling reference
            quadratic_ref = [t_min * (n / n_min) ** 2 for n in population_sizes]
            ax3.loglog(population_sizes, quadratic_ref, "k:", alpha=0.5, label="O(n¬≤) reference")

        ax3.set_xlabel("Population Size")
        ax3.set_ylabel("Execution Time (seconds)")
        ax3.set_title("Scaling Behavior Analysis")
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # Plot 4: Performance improvement percentage
        improvements = [comp.time_improvement for comp in self.performance_comparisons]
        ax4.bar(range(len(population_sizes)), improvements, alpha=0.8, color="blue")
        ax4.set_xlabel("Population Size")
        ax4.set_ylabel("Time Improvement (%)")
        ax4.set_title("Performance Improvement Percentage")
        ax4.set_xticks(range(len(population_sizes)))
        ax4.set_xticklabels([str(p) for p in population_sizes])
        ax4.grid(True, alpha=0.3)

        # Add improvement annotations
        for i, (_pop, improvement) in enumerate(zip(population_sizes, improvements)):
            ax4.annotate(
                f"{improvement:.1f}%", (i, improvement), textcoords="offset points", xytext=(0, 5), ha="center"
            )

        plt.tight_layout()

        # Save plot
        plot_file = self.output_dir / "performance_comparison.png"
        plt.savefig(plot_file, dpi=300, bbox_inches="tight")
        logger.info(f"Performance plots saved to {plot_file}")

        plt.show()

    @beartype
    def print_summary_report(self) -> None:
        """Print a comprehensive summary report to console."""
        if not self.performance_comparisons and not self.benchmark_results:
            print("No benchmark results available. Run benchmark first.")
            return

        report = self.generate_performance_report()

        # Handle diagnostic case where vectorized implementation failed
        if not self.performance_comparisons:
            self._create_diagnostic_report()
            return

        print("\n" + "=" * 80)
        print("VECTORIZED LAYER 2 PERFORMANCE BENCHMARK REPORT")
        print("=" * 80)

        print("\nBenchmark Configuration:")
        print(f"  Population sizes tested: {report['benchmark_summary']['population_sizes']}")
        print(f"  Generations per run: {report['benchmark_summary']['generations_per_run']}")
        print(f"  Runs per configuration: {report['benchmark_summary']['runs_per_configuration']}")
        print(f"  Total benchmark runs: {report['benchmark_summary']['total_benchmark_runs']}")

        print("\nOverall Performance Results:")
        perf = report["performance_results"]
        print(f"  Mean speedup: {perf['mean_speedup']:.2f}x")
        print(f"  Maximum speedup: {perf['max_speedup']:.2f}x")
        print(f"  Minimum speedup: {perf['min_speedup']:.2f}x")
        print(f"  Mean time improvement: {perf['mean_time_improvement_percent']:.1f}%")
        print(f"  Mathematical equivalence rate: {perf['mathematical_equivalence_rate']:.1%}")

        print("\nDetailed Results by Population Size:")
        print(f"{'Size':<8} {'Speedup':<10} {'Improve%':<10} {'Vec Time':<12} {'Seq Time':<12} {'Equiv':<8}")
        print("-" * 70)

        for comp in report["detailed_comparisons"]:
            print(
                f"{comp['population_size']:<8} "
                f"{comp['speedup_ratio']:<10.2f} "
                f"{comp['time_improvement_percent']:<10.1f} "
                f"{comp['vectorized_time']:<12.3f} "
                f"{comp['sequential_time']:<12.3f} "
                f"{'‚úì' if comp['mathematical_equivalence'] else '‚úó':<8}"
            )

        print("\nValidation Results:")
        val = report["validation_results"]
        print(f"  All runs mathematically equivalent: {'‚úì' if val['all_runs_mathematically_equivalent'] else '‚úó'}")
        print(f"  Performance improvements consistent: {'‚úì' if val['performance_improvements_consistent'] else '‚úó'}")
        print(f"  Expected performance gains achieved: {'‚úì' if val['expected_performance_gains_achieved'] else '‚úó'}")

        print("\nConclusion:")
        if perf["mean_speedup"] >= 2.0 and perf["mathematical_equivalence_rate"] >= 0.95:
            print("  üéâ VECTORIZED LAYER 2 SUCCESSFULLY VALIDATED!")
            print(f"  - Achieved {perf['mean_speedup']:.1f}x average speedup")
            print(f"  - Maintained {perf['mathematical_equivalence_rate']:.1%} mathematical equivalence")
            print("  - Ready for production use in large-scale simulations")
        else:
            print("  ‚ö†Ô∏è  Performance or validation issues detected")
            if perf["mean_speedup"] < 2.0:
                print(f"  - Speedup {perf['mean_speedup']:.1f}x is below expected 2.0x minimum")
            if perf["mathematical_equivalence_rate"] < 0.95:
                print(
                    f"  - Mathematical equivalence {perf['mathematical_equivalence_rate']:.1%} is below 95% threshold"
                )

        print("=" * 80)


@beartype
def run_performance_demo(
    population_sizes: list[int] | None = None,
    generations: int = 50,
    runs_per_config: int = 3,
    output_dir: str = "experiments/performance_results",
    plot_results: bool = True,
    save_results: bool = True,
) -> dict[str, Any]:
    """
    Run the complete vectorized Layer 2 performance demonstration.

    Parameters
    ----------
    population_sizes : list[int] | None
        Population sizes to benchmark (default: [100, 500, 1000, 2000, 5000])
    generations : int
        Number of generations per run
    runs_per_config : int
        Number of runs per configuration
    output_dir : str
        Directory to save results
    plot_results : bool
        Whether to generate performance plots
    save_results : bool
        Whether to save results to file

    Returns
    -------
    dict[str, Any]
        Complete performance report

    Examples
    --------
    >>> # Quick performance test
    >>> report = run_performance_demo(
    ...     population_sizes=[100, 500, 1000],
    ...     generations=20,
    ...     runs_per_config=2
    ... )
    >>> print(f"Average speedup: {report['performance_results']['mean_speedup']:.2f}x")

    >>> # Full benchmark suite
    >>> report = run_performance_demo()
    >>> # Results automatically saved and plotted
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    # Create benchmark instance
    benchmark = PerformanceBenchmark(
        population_sizes=population_sizes,
        generations=generations,
        runs_per_config=runs_per_config,
        output_dir=output_dir,
    )

    # Run the complete benchmark
    benchmark.run_full_benchmark()

    # Generate and display results
    benchmark.print_summary_report()

    # Generate performance report
    report = benchmark.generate_performance_report()

    # Save results if requested
    if save_results:
        benchmark.save_results()

    # Generate plots if requested
    if plot_results:
        benchmark.plot_performance_comparison()

    return report


def main() -> None:
    """Command-line interface for performance demonstration."""
    parser = argparse.ArgumentParser(
        description="Vectorized Layer 2 Performance Demonstration",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quick test with small populations
  uv run experiments/vectorized_performance_demo.py --quick

  # Full benchmark suite
  uv run experiments/vectorized_performance_demo.py --full

  # Custom population sizes
  uv run experiments/vectorized_performance_demo.py --populations 100 500 1000

  # Extended benchmark
  uv run experiments/vectorized_performance_demo.py --generations 100 --runs 5
        """,
    )

    parser.add_argument("--quick", action="store_true", help="Run quick benchmark with small populations")

    parser.add_argument("--full", action="store_true", help="Run full benchmark suite (default)")

    parser.add_argument(
        "--populations", type=int, nargs="+", help="Population sizes to test (default: [100, 500, 1000, 2000, 5000])"
    )

    parser.add_argument("--generations", type=int, default=50, help="Number of generations per run (default: 50)")

    parser.add_argument("--runs", type=int, default=3, help="Number of runs per configuration (default: 3)")

    parser.add_argument(
        "--output-dir",
        type=str,
        default="experiments/performance_results",
        help="Output directory for results (default: experiments/performance_results)",
    )

    parser.add_argument("--no-plots", action="store_true", help="Skip generating performance plots")

    parser.add_argument("--no-save", action="store_true", help="Skip saving results to file")

    args = parser.parse_args()

    # Determine population sizes
    if args.quick:
        population_sizes = [100, 500, 1000]
        generations = 20
        runs = 2
    elif args.populations:
        population_sizes = args.populations
        generations = args.generations
        runs = args.runs
    else:
        # Default full benchmark
        population_sizes = [100, 500, 1000, 2000, 5000]
        generations = args.generations
        runs = args.runs

    print("Starting Vectorized Layer 2 Performance Demonstration")
    print(f"Population sizes: {population_sizes}")
    print(f"Generations: {generations}")
    print(f"Runs per configuration: {runs}")
    print("-" * 60)

    # Run the performance demonstration
    report = run_performance_demo(
        population_sizes=population_sizes,
        generations=generations,
        runs_per_config=runs,
        output_dir=args.output_dir,
        plot_results=not args.no_plots,
        save_results=not args.no_save,
    )

    print("\nPerformance demonstration completed successfully!")
    print(f"Average speedup achieved: {report['performance_results']['mean_speedup']:.2f}x")
    print(f"Results saved to: {args.output_dir}")


if __name__ == "__main__":
    main()
